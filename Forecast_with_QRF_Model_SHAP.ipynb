{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shapley Additive Explanations (SHAP) of QRF Predictions\n",
    "\n",
    "Version 21 December 2022, Selina Kiefer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input: csv-file, pt-file\n",
    "continuous timeseries of input data (e.g. statistics of meteorological predictor fields), Quantile Random Forests model in pt-format\n",
    "### Output: png-file\n",
    "results of SHAP analysis plotted in png-format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the paths' to the defined functions, the style sheet for plotting and tthe configuration file and set its name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the defined functions.\n",
    "PATH_defined_functions = './Defined_Functions/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path and name of the style file which should be used for plotting.\n",
    "style_file_for_plotting = './Style_File_Matplotlib.mplstyle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path and name of the configuration file.\n",
    "PATH_configurations = './Configuration_Files/'\n",
    "ifile_configurations = 'Configurations_QRF_SHAP.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the necessary python packages and functions\n",
    "Nothing needs to be changed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary python packages.\n",
    "import yaml\n",
    "import calendar\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from skranger.ensemble import RangerForestRegressor\n",
    "\n",
    "import shap\n",
    "from shap import TreeExplainer\n",
    "from skranger.utils.shap import shap_patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary python packages and functions.\n",
    "import sys\n",
    "sys.path.insert(1, PATH_defined_functions)\n",
    "from read_in_csv_data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the style sheet for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the style sheet to be used by matplotlib for plotting. This will update the plotting\n",
    "# parameters to e.g. have the right font, font size and figure size. The latter is adjusted to\n",
    "# the textwidth of the LaTeX-document in order to avoid re-scaling the plot and changing \n",
    "# thereby the font size again.\n",
    "plt.style.use(style_file_for_plotting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the configuration file and the data specified in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the configuration file (nothing needs to be changed here).\n",
    "with open(PATH_configurations+ifile_configurations) as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the input data and remove any unnamed columns as well as the index column (nothing \n",
    "# needs to be changed here).\n",
    "df_input_data = read_in_csv_data(config['PATH_input_data'], config['ifile_input_data'])\n",
    "df_input_data = df_input_data.loc[:, ~df_input_data.columns.str.contains('^Unnamed')]\n",
    "df_input_data = df_input_data.drop(['index'], axis =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the name of the columns containing the time and the variables of the input data.\n",
    "time_column_name_input_data = df_input_data.columns[0]\n",
    "var_column_name_input_data = df_input_data.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that everything is selected correctly (nothing needs to be changed here).\n",
    "print('Predictors used for training the ML model: ')\n",
    "print(var_column_name_input_data)\n",
    "print('Name of the column containing the time: ')\n",
    "print(time_column_name_input_data)\n",
    "print('Dataframe containing the predictors: ')\n",
    "df_input_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the input data for the SHAP analysis\n",
    "From here on, nothing needs to be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list containing the names of the input features is created. \n",
    "df_input_features = df_input_data.drop([time_column_name_input_data], axis=1)\n",
    "input_features = df_input_features.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to extract the different time periods for the SHAP analysis, the index of the dataframe\n",
    "# containing the input data is set to the time. The time column is converted beforehand into a\n",
    "# datetime-object.\n",
    "df_input_data[time_column_name_input_data] = pd.to_datetime(df_input_data[time_column_name_input_data])\n",
    "df_input_data = df_input_data.set_index(time_column_name_input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the quantiles used for predicting by the QRF models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For predicting with the QRF-model, a list with the desired quantiles is created. \n",
    "if config['distributed_evenly']:\n",
    "    list_quantiles_qrf = list(np.round(np.linspace(0, 1,config['number_of_quantiles']), decimals=2))\n",
    "else:\n",
    "    list_quantiles_qrf = config['list_quantiles_qrf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the mean prediction of the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At first, the QRF model is loaded and the start as well as the end of the winter to be predicted set. Then, this\n",
    "# winter is excluded from the input data forming the training dataset. In a next step, a probabilistic forecast on \n",
    "# the training dataset is done by QRF model. This forecast is then averaged over all quantiles and afterwards over\n",
    "# all predicted days creating the mean prediction of the training dataset.\n",
    "predictions = []\n",
    "forecast_dates = []\n",
    "winterly_values_all_winters = []\n",
    "\n",
    "quantile_regresssion_forest = torch.load(config['PATH_model']+config['ifile_name_model'])\n",
    "\n",
    "start_winter_to_be_excluded = datetime(config['start_year_of_winter'], config['start_month_winter'], config['start_day_winter'])\n",
    "\n",
    "end_winter_to_be_excluded = datetime(config['start_year_of_winter']+1, config['end_month_winter'], config['end_day_winter'])\n",
    "\n",
    "start_winter_to_be_excluded_minus_1_day = start_winter_to_be_excluded - timedelta(days=1)\n",
    "end_winter_to_be_excluded_plus_1_day = end_winter_to_be_excluded + timedelta(days=1)\n",
    "\n",
    "df_X_train = df_input_data.loc[(df_input_data.index < start_winter_to_be_excluded_minus_1_day) | (df_input_data.index > end_winter_to_be_excluded_plus_1_day)]    \n",
    "df_X_train = df_X_train.reset_index()\n",
    "df_X_train = df_X_train.drop([time_column_name_input_data], axis=1)\n",
    "X_train = np.array(df_X_train)   \n",
    "\n",
    "mean_prediction_value = np.mean(quantile_regresssion_forest.predict_quantiles(X_train, quantiles=list_quantiles_qrf), axis=1)\n",
    "mean_prediction_value = np.mean(mean_prediction_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculation of the actual prediction of given time periods as well as calculating and plotting SHAP analysis of these time periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, the SHAP analysis takes place. At first, the desired time period for the analysis is extracted from the input\n",
    "# data. Then, the probabilistic prediction of the QRF model for this time period is obtained. This prediction is\n",
    "# first averaged over all ensemble members and then over all predicted days creating the actual prediction of the QRF\n",
    "# for the time period. As a next step, the SHAP analysis (with TreeExplainer) is done for every day in the time\n",
    "#  period(.shap_values). Then, the mean and the standard deviation of these SHAP values are calculated and the top 5 \n",
    "# positive and negative contributing predictors plotted as a bar plot together with the mean prediction \n",
    "# of the training dataset and the actual prediction of the QRF for the time period. The difference between the latter\n",
    "# 2 is roughly the same as the sum over all SHAP values. It does not fit perfectly well since here averages over\n",
    "# probabilistic forecasts are used instead of deterministic forecasts. This procedure is then done for every \n",
    "# specified time period in the configuration file separately. \n",
    "list_str_times = []\n",
    "list_actual_prediction_value = []\n",
    "list_temperature_periods = []\n",
    "\n",
    "for i in range(len(config['start_dates_for_SHAP'])):\n",
    "    \n",
    "    predictions = []\n",
    "    forecast_dates = []\n",
    "    winterly_values_all_winters = []\n",
    "    \n",
    "    start_period = datetime.strptime(config['start_dates_for_SHAP'][i], '%Y-%m-%d')\n",
    "    end_period = datetime.strptime(config['end_dates_for_SHAP'][i], '%Y-%m-%d')\n",
    "    \n",
    "    list_str_times.append(start_period.strftime('%d %b %Y')+' - '+end_period.strftime('%d %b %Y'))\n",
    "\n",
    "    temperature_period = config['time_period_name'][i]\n",
    "    list_temperature_periods.append(temperature_period)\n",
    "    \n",
    "    start_period_minus_1_day = start_period - timedelta(days=1)-timedelta(days=config['lead_time'])\n",
    "    end_period_plus_1_day = end_period + timedelta(days=1)-timedelta(days=config['lead_time'])\n",
    "\n",
    "    df_X_val = df_input_data.loc[(df_input_data.index > start_period_minus_1_day) & (df_input_data.index < end_period_plus_1_day)]    \n",
    "    df_X_val = df_X_val.reset_index()\n",
    "        \n",
    "    X_val = df_X_val.drop([time_column_name_input_data], axis=1)\n",
    "\n",
    "    actual_prediction_value = np.mean(quantile_regresssion_forest.predict_quantiles(X_val, quantiles=list_quantiles_qrf), axis=1)\n",
    "    actual_prediction_value = np.mean(actual_prediction_value)  \n",
    "    list_actual_prediction_value.append(actual_prediction_value)\n",
    "    \n",
    "    with shap_patch():\n",
    "        explainer = TreeExplainer(model=quantile_regresssion_forest)\n",
    "    \n",
    "    daily_values = explainer.shap_values(X_val)\n",
    "    time_period_values_mean = np.mean(daily_values, axis=0)\n",
    "    time_period_values_std = np.std(daily_values, axis=0)\n",
    "    \n",
    "    df_shap_values = pd.DataFrame(time_period_values_mean)\n",
    "    df_shap_values['Input Features'] = input_features\n",
    "    df_shap_values['Standard Deviation'] = time_period_values_std\n",
    "\n",
    "    df_shap_values_sorted = df_shap_values.sort_values(0, ascending=False)\n",
    "    shap_mean_values_sorted = df_shap_values_sorted[0]\n",
    "    shap_std_values_sorted = df_shap_values_sorted['Standard Deviation']\n",
    "    input_features_sorted = df_shap_values_sorted['Input Features']\n",
    "\n",
    "    #text_for_on_plot = 'Mean Prediction: ' + str(np.round(mean_prediction_value, decimals=2)) +' '+config['unit_of_ground_truth_and_prediction']+'\\nActual Prediction: ' + str(np.round(actual_prediction_value, decimals=2)) +' '+config['unit_of_ground_truth_and_prediction']+'\\nDifference: ' + str(np.round(np.round(actual_prediction_value, decimals=2)-np.round(mean_prediction_value, decimals=2), decimals=2)) +' '+config['unit_of_ground_truth_and_prediction']\n",
    "    # don't plot the text on the plot for paper, rather create a table from the csv-file created later.\n",
    "    \n",
    "    fig,ax = plt.subplots()\n",
    "    #plt.text(0,-0.35, text_for_on_plot, transform=ax.transAxes, verticalalignment='bottom')\n",
    "    plt.barh(input_features_sorted[0:5], shap_mean_values_sorted[0:5], xerr=shap_std_values_sorted[0:5], label=input_features_sorted[0:5], color = 'r', alpha=0.6, ecolor='darkred', capsize=3)\n",
    "    plt.barh(input_features_sorted[len(input_features)-5:len(input_features)], shap_mean_values_sorted[len(input_features)-5:len(input_features)], xerr=shap_std_values_sorted[len(input_features)-5:len(input_features)], label=input_features_sorted[len(input_features)-5:len(input_features)], color = 'b', alpha=0.6, ecolor='darkblue', capsize=3)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title(config['model_name']+', '+temperature_period+'\\n '+start_period.strftime('%d %b %Y')+' - '+end_period.strftime('%d %b %Y'))\n",
    "    plt.xlabel('SHAP Values')\n",
    "    plt.savefig(config['PATH_plots']+'SHAP_'+config['model_name']+'_'+config['ground_truth']+'_'+config['location_ground_truth']+'_input_'+config['input_data']+'_'+config['location_input']+'_Lead_'+str(config['lead_time'])+'d_'+temperature_period.replace(' ', '_')+'_'+start_period.strftime('%d_%b_%Y')+'_'+end_period.strftime('%d_%b_%Y')+'.png', bbox_inches = 'tight')  \n",
    "    #plt.show() # not used since the size of the jupyter notebook is really big\n",
    "    plt.close() # only used since the size of the jupyter notebook is really big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pandas dataframe with the mean and actual prediction as well as the difference.\n",
    "df_details_shap_analysis = pd.DataFrame(list_str_times)\n",
    "df_details_shap_analysis['Temperature Period'] = list_temperature_periods\n",
    "df_details_shap_analysis['Mean Pred.'] = mean_prediction_value\n",
    "df_details_shap_analysis['Actual Pred.'] = list_actual_prediction_value\n",
    "df_details_shap_analysis['Difference'] = list_actual_prediction_value-mean_prediction_value\n",
    "df_details_shap_analysis = df_details_shap_analysis.rename(columns={0 : 'time'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only the analysis of the cold periods (this is done for a nice representation in the paper).\n",
    "df_cold_periods = df_details_shap_analysis.where(df_details_shap_analysis['Temperature Period'] == 'Cold Period')\n",
    "df_cold_periods = df_cold_periods.dropna()\n",
    "df_cold_periods [' '] = list(map(chr, range(97, 97+(len(df_cold_periods))))) # create a list of lowercase letters\n",
    "df_cold_periods = df_cold_periods.drop(['time', 'Temperature Period'], axis=1)\n",
    "df_cold_periods = df_cold_periods.set_index(' ')\n",
    "df_cold_periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in csv-format.\n",
    "df_cold_periods.to_csv(config['PATH_plots']+'SHAP_'+config['model_name']+'_'+config['ground_truth']+'_'+config['location_ground_truth']+'_input_'+config['input_data']+'_'+config['location_input']+'_Lead_'+str(config['lead_time'])+'d_cold_periods'+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the same for the warm periods.\n",
    "# Take only the analysis of the warm periods (this is done for a nice representation in the paper).\n",
    "df_warm_periods = df_details_shap_analysis.where(df_details_shap_analysis['Temperature Period'] == 'Warm Period')\n",
    "df_warm_periods = df_warm_periods.dropna()\n",
    "df_warm_periods [' '] = list(map(chr, range(97, 97+(len(df_warm_periods))))) # create a list of lowercase letters\n",
    "df_warm_periods = df_warm_periods.drop(['time', 'Temperature Period'], axis=1)\n",
    "df_warm_periods = df_warm_periods.set_index(' ')\n",
    "df_warm_periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in csv-format.\n",
    "df_cold_periods.to_csv(config['PATH_plots']+'SHAP_'+config['model_name']+'_'+config['ground_truth']+'_'+config['location_ground_truth']+'_input_'+config['input_data']+'_'+config['location_input']+'_Lead_'+str(config['lead_time'])+'d_warm_periods'+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End of Program"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
